{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Stemming</a></span></li><li><span><a href=\"#Lemmatisation\" data-toc-modified-id=\"Lemmatisation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Lemmatisation</a></span></li><li><span><a href=\"#Stop-words\" data-toc-modified-id=\"Stop-words-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Stop words</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of segmenting text into relevant words is called tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T17:34:16.336744Z",
     "start_time": "2021-03-29T17:34:12.116745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\goutham-rog\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\goutham-rog\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\goutham-rog\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\goutham-rog\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\goutham-rog\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T17:34:18.501405Z",
     "start_time": "2021-03-29T17:34:18.487433Z"
    }
   },
   "outputs": [],
   "source": [
    "text = 'we will look into the core components that are relevant to the language in computational linguistics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T17:34:18.861281Z",
     "start_time": "2021-03-29T17:34:18.788263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Goutham-\n",
      "[nltk_data]     ROG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'will',\n",
       " 'look',\n",
       " 'into',\n",
       " 'the',\n",
       " 'core',\n",
       " 'components',\n",
       " 'that',\n",
       " 'are',\n",
       " 'relevant',\n",
       " 'to',\n",
       " 'the',\n",
       " 'language',\n",
       " 'in',\n",
       " 'computational',\n",
       " 'linguistics']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have complex words like {what's} simple tokenization wont work<br/>\n",
    "work around: split the text into words by white spaces and repalce all punctuations with nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T17:37:56.679350Z",
     "start_time": "2021-03-29T17:37:56.655347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whats', 'the', 'name', 'of', 'the', 'city']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "text = 'What\\'s the name of the city?'\n",
    "words = text.split()\n",
    "table = str.maketrans('','',string.punctuation)\n",
    "[w.translate(table) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T17:38:16.199330Z",
     "start_time": "2021-03-29T17:38:16.187331Z"
    }
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming helps in reducing the vocabulary present in the documents, which saves a lot of computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T17:39:48.253846Z",
     "start_time": "2021-03-29T17:39:48.238848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', 'walk', 'ate', 'eat', 'walk']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['walked', 'walking', 'ate', 'eating', 'walks']\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "[porter.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatisation does very similar to stemming as it removes inflection and suffixes to convert words into their root words .meaning and context can be lost in the Stemming, lemmatisation preserves the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T17:51:25.753161Z",
     "start_time": "2021-03-29T17:51:18.323453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Goutham-\n",
      "[nltk_data]     ROG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['walked', 'walking', 'ate', 'eating', 'walk']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "[lemm.lemmatize(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T18:04:17.948399Z",
     "start_time": "2021-03-29T18:04:17.923417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Goutham-\n",
      "[nltk_data]     ROG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
